{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDAPjh38Qya7"
      },
      "source": [
        "#ML Security Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy4rRaqyRcTb"
      },
      "source": [
        "The notebook is organized as follows: first, we do the training and testing of a simple I-FGSM on a 2D toy case. After that, we define the helper functions and the attacks for the model ensemble, so that we can optimize the real attack on the 3-model ensemble. In the end, we evaluated the transferability of the generated adversarial examples to other 7 models, and we record the number of successful transfers for each type of attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTI6I81jexB-",
        "outputId": "2b097139-8cfa-4ca2-bf5c-402f3b31df00"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/RobustBench/robustbench.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwjKXn-QbOsh"
      },
      "source": [
        "#I-FGSM 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "bbYxLLvhP398",
        "outputId": "e7f25e42-255e-4fe1-a248-303d9d6a608c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_features, n_outputs):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_features, 50)\n",
        "        self.fc2 = nn.Linear(50, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def margin_loss(logits, target):\n",
        "    logits = logits.squeeze()  # remove batch size (in this small example is always 1)\n",
        "    true_class_logit = logits[target.item()]\n",
        "\n",
        "    masked_logits = logits.detach().clone()\n",
        "    masked_logits[target.item()] = -torch.inf\n",
        "\n",
        "    max_other_logit = masked_logits.max()\n",
        "\n",
        "    return max_other_logit - true_class_logit\n",
        "\n",
        "def train(model, X_train, y_train, optimizer, criterion, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_train)\n",
        "        loss = criterion(logits, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def fgsm_attack(model, x, y, loss_fct, epsilon=0.1):\n",
        "    x.requires_grad = True\n",
        "    logits = model(x)\n",
        "    loss = loss_fct(logits, y)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    perturbation = epsilon * x.grad.data.sign()\n",
        "    x_adv = x + perturbation\n",
        "    return x_adv.detach()\n",
        "\n",
        "def pgd(model, x, y_true, loss_fct, epsilon=0.1, max_iter=20):\n",
        "    x_start = x.detach().clone()\n",
        "    print('confidence before', model(x_start))\n",
        "    for _ in range(max_iter):\n",
        "        with torch.no_grad():\n",
        "            if model(x_start).argmax() != y_true:\n",
        "                break\n",
        "        x_start = fgsm_attack(model, x_start, y_true, loss_fct, epsilon)\n",
        "\n",
        "    print('confidence after', model(x_start))\n",
        "\n",
        "    return x_start.detach()\n",
        "\n",
        "def main_fgsm_2d():\n",
        "    ################### Train a simple fully connected neural network\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    seed = 42\n",
        "    X, y = make_blobs(n_samples=300, centers=3, n_features=2, random_state=seed)\n",
        "    X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "\n",
        "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "\n",
        "    model = Net(X_train.shape[1], len(torch.unique(y))).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train(model, X_train, y_train, optimizer, criterion, epochs=1000)\n",
        "\n",
        "    ################### Compute adv examples against the FC neural network\n",
        "    epsilon = 0.1\n",
        "    indices = np.random.choice(len(X_test), 3, replace=False)\n",
        "    adv_examples = []\n",
        "\n",
        "    for i in indices:\n",
        "        x = X_test[i].unsqueeze(0)\n",
        "        y_true = y_test[i].unsqueeze(0)\n",
        "        x_adv = pgd(model, x, y_true, margin_loss, epsilon) # try also fgsm!\n",
        "        adv_examples.append((x.squeeze().cpu().detach().numpy(), x_adv.squeeze().cpu().numpy()))\n",
        "\n",
        "    ################### Plot decision regions and adversarial examples\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    h = 0.02\n",
        "    xx, yy = np.meshgrid(np.arange(-0.5, 1.5, h), np.arange(-0.5, 1.5, h))\n",
        "    grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        Z = model(grid_tensor).argmax(dim=1).cpu().numpy()\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.rainbow)\n",
        "\n",
        "    plt.scatter(X[:, 0].cpu(), X[:, 1].cpu(), c=y.cpu(), cmap=plt.cm.rainbow,\n",
        "              edgecolors='k', alpha=0.6, label='Dataset')\n",
        "\n",
        "    for orig, adv in adv_examples:\n",
        "        plt.plot([orig[0], adv[0]], [orig[1], adv[1]], 'k--', lw=1)\n",
        "        plt.scatter(orig[0], orig[1], c='green', s=80, edgecolors='k', zorder=3)\n",
        "        plt.scatter(adv[0], adv[1], c='red', s=300, edgecolors='k', marker='*', zorder=3)\n",
        "        plt.gca().add_patch(plt.Rectangle(\n",
        "            (orig[0]-epsilon, orig[1]-epsilon), 2*epsilon, 2*epsilon,\n",
        "            linewidth=1, edgecolor='black', facecolor='none', linestyle='-'\n",
        "        ))\n",
        "\n",
        "    legend_elements = [\n",
        "        plt.Line2D([0], [0], color='k', linestyle='--', label='Perturbation'),\n",
        "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Original'),\n",
        "        plt.Line2D([0], [0], marker='*', color='w', markerfacecolor='red', markersize=15, label='Perturbed point'),\n",
        "        plt.Line2D([0],[0], color='black', linestyle='-', label=r\"$\\ell_{\\infty}$-norm\")\n",
        "    ]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='best')\n",
        "    plt.show()\n",
        "\n",
        "main_fgsm_2d()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P45cElTNrfn-"
      },
      "source": [
        "#Optimize the attack against 3 models simultaneously"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OdNhNU2_Uo9"
      },
      "source": [
        "This section details the implementation of three distinct adversarial attacks on the ensemble model:\n",
        "- fgsm_ensemble with averaged logits margin loss\n",
        "- pgd_ensemble with sum of margin losses\n",
        "- mi_fgsm_ensemble with cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h64MDS1CJgbS"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def ensemble_predict(models, x):\n",
        "    \"\"\"\n",
        "      Compute the avareged logits for each model.\n",
        "\n",
        "      Use this function only for inference, avoid for computing\n",
        "      adversarial examples because uses torch.no_grad()\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "       logits = [model(x) for model in models]\n",
        "       avg_logits = torch.mean(torch.stack(logits), dim=0)\n",
        "    return avg_logits.argmax().item(), avg_logits\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def margin_loss(logits, target):\n",
        "    logits = logits.squeeze()\n",
        "    true_class_logit = logits[target.item()]\n",
        "    masked_logits = logits.detach().clone()\n",
        "    masked_logits[target.item()] = -torch.inf\n",
        "    max_other_logit = masked_logits.max()\n",
        "    return max_other_logit - true_class_logit\n",
        "\n",
        "def plot_original_adv_images(x, x_adv, y_pred, y_pred_adv):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    axes[0].imshow(np.transpose(x.squeeze().cpu().numpy(), (1, 2, 0)))\n",
        "    axes[0].set_title(f\"Original: {CIFAR10_CLASSES[y_pred]}\")\n",
        "    axes[0].axis(\"off\")\n",
        "    rect1 = patches.Rectangle((0, 0), 1, 1, linewidth=5, edgecolor=\"black\", facecolor='none', transform=axes[0].transAxes)\n",
        "    axes[0].add_patch(rect1)\n",
        "\n",
        "    axes[1].imshow(np.transpose(x_adv.squeeze().cpu().detach().numpy(), (1, 2, 0)))\n",
        "    axes[1].set_title(f\"Adv: {CIFAR10_CLASSES[y_pred_adv]}\")\n",
        "    axes[1].axis(\"off\")\n",
        "    rect2 = patches.Rectangle((0, 0), 1, 1, linewidth=5, edgecolor=\"black\", facecolor='none', transform=axes[1].transAxes)\n",
        "    axes[1].add_patch(rect2)\n",
        "\n",
        "    perturbation = np.abs((x_adv - x).squeeze().cpu().detach().numpy())\n",
        "    perturbation /= perturbation.max()\n",
        "    axes[2].imshow(np.transpose(perturbation, (1, 2, 0)))\n",
        "    axes[2].set_title(\"Perturbation\")\n",
        "    axes[2].axis(\"off\")\n",
        "    rect3 = patches.Rectangle((0, 0), 1, 1, linewidth=5, edgecolor=\"black\", facecolor='none', transform=axes[2].transAxes)\n",
        "    axes[2].add_patch(rect3)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh9t28u2nQ27"
      },
      "source": [
        "Method A - FGSM Ensemble attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6A20SbKuOiJf"
      },
      "outputs": [],
      "source": [
        "def fgsm_ensemble(models, x, y_true, eta=8/255):\n",
        "    '''\n",
        "      A standard FGSM, the only difference is that the the loss is computed\n",
        "      starting from the average logits of the ensemble.\n",
        "    '''\n",
        "    x_adv = x.clone().detach()\n",
        "    x_adv.requires_grad = True\n",
        "\n",
        "    logits = torch.mean(torch.stack([model(x_adv) for model in models]), dim=0)\n",
        "    loss = margin_loss(logits, y_true)\n",
        "\n",
        "    grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "    x_adv = x_adv.detach() + eta * torch.sign(grad.detach())\n",
        "    x_adv = torch.clamp(x_adv, min=0, max=1).detach()\n",
        "\n",
        "    y_pred, _ = ensemble_predict(models, x_adv)\n",
        "\n",
        "    return x_adv, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1WLXr1cnUl2"
      },
      "source": [
        "Method B - I-FGSM Ensemble attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mL1HZwXF_SaV"
      },
      "outputs": [],
      "source": [
        "def pgd_ensemble(models, x, y_true, epsilon=8/255, eta=3/255, max_iter=20):\n",
        "    \"\"\"\n",
        "      A standard I-FGSM/PGD implementation, the only difference is that the\n",
        "      loss is computed using sum of the margin losses.\n",
        "    \"\"\"\n",
        "    x_adv = x.clone().detach()\n",
        "    x_orig = x.clone().detach()\n",
        "    y_pred = y_true\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        x_adv.requires_grad = True\n",
        "        losses = []\n",
        "\n",
        "        # Custom loss: sum of margin losses\n",
        "        for model in models:\n",
        "            logits = model(x_adv)\n",
        "            loss = margin_loss(logits, y_true)\n",
        "            losses.append(loss)\n",
        "        total_loss = sum(losses)\n",
        "\n",
        "        if DEBUG:\n",
        "          print(f\"Iteration {i}: Total Loss = {total_loss.item():.4f}\")\n",
        "\n",
        "        for model in models:\n",
        "            model.zero_grad()\n",
        "        total_loss.backward()\n",
        "\n",
        "        # FGSM update\n",
        "        x_adv = x_adv + eta * x_adv.grad.data.sign()\n",
        "\n",
        "        # Projection l-inf norm + [0,1] constraint\n",
        "        delta = torch.clamp(x_adv - x_orig, min=-epsilon, max=epsilon)\n",
        "        x_adv = torch.clamp(x_orig + delta, min=0, max=1).detach()\n",
        "\n",
        "        # Early stopping when during the optimization, the ensemble have already\n",
        "        # misclassified the adversarial example\n",
        "        y_pred, _ = ensemble_predict(models, x_adv)\n",
        "        if y_pred != y_true:\n",
        "            print(f'Early stopping at iter {i}: ensemble misclassification!')\n",
        "            break\n",
        "\n",
        "    return x_adv, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_g6sG9TnYQ6"
      },
      "source": [
        "Method C - MI-FGSM Ensemble attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wHZCEXh4_ZC2"
      },
      "outputs": [],
      "source": [
        "def mi_fgsm_ensemble(models, x, y_true, epsilon=8/255, eta=3/255, max_iter=10, mu=0.9, weights=None):\n",
        "    \"\"\"\n",
        "      MI-FGSM: uses momentum and a custom fused logit loss\n",
        "    \"\"\"\n",
        "    x_adv = x.clone().detach()\n",
        "    momentum = torch.zeros_like(x_adv)\n",
        "\n",
        "    # Optional: pass ensemble weights\n",
        "    if weights is None:\n",
        "        weights = [1.0 / len(models)] * len(models)\n",
        "    else:\n",
        "        weights = [w / sum(weights) for w in weights]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        x_adv.requires_grad = True\n",
        "\n",
        "        # Custom fused logits loss (check the paper)\n",
        "        fused_logits = sum(weight * model(x_adv) for model, weight in zip(models, weights))\n",
        "        loss = F.cross_entropy(fused_logits, y_true)\n",
        "\n",
        "        for model in models:\n",
        "            model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Normalize the gradient\n",
        "        grad = x_adv.grad.data\n",
        "        grad_norm = torch.sum(torch.abs(grad), dim=(1, 2, 3), keepdim=True) + 1e-12\n",
        "        normalized_grad = grad / grad_norm\n",
        "\n",
        "        # MI-FGSM update\n",
        "        momentum = mu*momentum + normalized_grad\n",
        "        x_adv = x_adv + eta*torch.sign(momentum)\n",
        "\n",
        "        # Projection l-inf norm + [0,1] constraint\n",
        "        delta = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
        "        x_adv = torch.clamp(x + delta, min=0, max=1).detach()\n",
        "\n",
        "        # Early stopping when during the optimization, the ensemble have already\n",
        "        # misclassified the adversarial example\n",
        "        y_pred, _ = ensemble_predict(models, x_adv)\n",
        "        if y_pred != y_true:\n",
        "            print(f'Early stopping at iter {i}: ensemble misclassification!')\n",
        "            break\n",
        "\n",
        "    return x_adv, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Na1GOrLnboM"
      },
      "source": [
        "In the next block, the defined attacks will be optimized against the model ensemble. The results will be saved in a dictionary called `res_adv_images`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "prZ8Owk5v4oC",
        "outputId": "06b19d1e-221d-4045-cd40-ce8bb8d8d130"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from robustbench.utils import load_model\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "\n",
        "DEBUG = 0\n",
        "SEED = 42\n",
        "NUM_SAMPLES = 100\n",
        "CIFAR10_CLASSES = ['aircraft', 'auto', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def optimize_against_ensemble():\n",
        "    # Set common seed for reproducibility\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # Load models from RobustBench\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    models = [\n",
        "        load_model(model_name='Sehwag2021Proxy_R18', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Addepalli2022Efficient_RN18', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Addepalli2021Towards_RN18', dataset='cifar10', threat_model='Linf').to(device).eval()\n",
        "    ]\n",
        "\n",
        "    # Load cifar 10\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Take the first NUM_SAMPLES samples (For the presentation, we included 100 samples)\n",
        "    subset = Subset(testset, range(NUM_SAMPLES))\n",
        "    testloader = DataLoader(subset, batch_size=1)\n",
        "\n",
        "    # Prepare the implemented attack\n",
        "    attacks = [\n",
        "        fgsm_ensemble,\n",
        "        pgd_ensemble,\n",
        "        mi_fgsm_ensemble\n",
        "    ]\n",
        "\n",
        "    # The dictionary `res_adv_images` will use the attack name as the key,\n",
        "    # and as the value, it will store a list of tuples in the form (adversarial image, true label).\n",
        "    # This dictionary will be useful in the next section, where we will transfer\n",
        "    # our adversarial examples to seven other models from RobustBench.\n",
        "    res_adv_images = defaultdict(list)\n",
        "\n",
        "    for attack in attacks:\n",
        "        # Variables useful for attack's statistics\n",
        "        total_adv_examples = 0\n",
        "        skipped_elements = 0\n",
        "\n",
        "        # Compute the attack for each sample in the dataset (limited to NUM_SAMPLES)\n",
        "        for i, (x, y_true) in enumerate(testloader):\n",
        "            print(f'Sample {i}/{NUM_SAMPLES}')\n",
        "            x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "            # Check ensemble prediction: consider optimizabile only those samples\n",
        "            # for which the ensemble predicts the correct label\n",
        "            y_pred, _ = ensemble_predict(models, x)\n",
        "\n",
        "            if y_pred != y_true: # Skip if the ensemble made an error\n",
        "                print(f'skipped element because y_pred={y_pred} and y_true={y_true}')\n",
        "                skipped_elements += 1\n",
        "                continue\n",
        "\n",
        "            # Optimize the adversarial example using the current attack\n",
        "            x_adv, y_pred_adv = attack(models, x, y_true, epsilon=8/255)\n",
        "\n",
        "            # Check whether the ensemble has predicted wrongly the sample.\n",
        "            # If this check is positive, then save the adversarial image\n",
        "            if y_pred_adv != y_true:\n",
        "                res_adv_images[attack.__name__].append((x_adv, y_true.item()))\n",
        "                total_adv_examples += 1\n",
        "\n",
        "            # Finally, plot the image regardless of whether it is adversarial or not\n",
        "            plot_original_adv_images(x, x_adv, y_pred, y_pred_adv)\n",
        "\n",
        "        print(f'FINISHED. Computed n.{total_adv_examples} adversarial examples, skipped n.{skipped_elements} samples')\n",
        "\n",
        "    return res_adv_images\n",
        "\n",
        "res_adv_images = optimize_against_ensemble()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL1uptX-Q2ya"
      },
      "source": [
        "# Transferability Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6ChLzKrEtJ"
      },
      "source": [
        "In this section, we take the results from the previous blocks and transfer the adversarial examples against the other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX2heR86JFEG",
        "outputId": "b8c0051e-48d2-4fe2-c14f-2c5d2b54bc3e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from robustbench.utils import download_gdrive, load_model\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def inference(model, image, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.to(device)\n",
        "        outputs = model(image)\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def transferability_test(res_adv_images):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Take 7 models from RobustBench\n",
        "    models = [\n",
        "        load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Bartoldson2024Adversarial_WRN-94-16', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Amini2024MeanSparse_S-WRN-94-16', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Bartoldson2024Adversarial_WRN-82-8', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Debenedetti2022Light_XCiT-M12', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Debenedetti2022Light_XCiT-S12', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "        load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_ddpm', dataset='cifar10', threat_model='Linf').to(device).eval(),\n",
        "    ]\n",
        "\n",
        "    # Similarly to the res_adv_images dictionary, the new transfer_success_rates\n",
        "    # dictionary will have as keys the name of the attack, and as values the\n",
        "    # transfer success rate for each model attacked\n",
        "    transfer_success_rates = defaultdict(list)\n",
        "\n",
        "    for attack, images in res_adv_images.items():\n",
        "        print(f'>>>>>>> Attack: {attack} - num_images = {len(images)}')\n",
        "        for model in models:\n",
        "            transfer_success = 0\n",
        "\n",
        "            # Iterate over each adversarial image for the current attack\n",
        "            for image, y_true in images:\n",
        "                model = model.to(device)\n",
        "                pred = inference(model, image, device)\n",
        "\n",
        "                # Add a success only whether the predicted label by the model\n",
        "                # is misclassified (so the attack was successfully transferred!)\n",
        "                transfer_success += int(pred[0] != y_true)\n",
        "\n",
        "            print(f'Model {model.__class__.__name__} - Transfer success = {transfer_success}')\n",
        "            transfer_success_rates[attack].append((model.__class__.__name__, transfer_success / len(images)))\n",
        "\n",
        "    return transfer_success_rates\n",
        "\n",
        "# tsr = transfer_success_rates\n",
        "tsr = transferability_test(res_adv_images)\n",
        "print(tsr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
